{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Measure of the Complexity of Neural Representations based on Partial Information Decomposition\n",
    "David A. Ehrlich, Andreas C. Schneider, Viola Priesemann, Michael Wibral, Abdullah Makkeh. TMLR 2023.\\\n",
    "Supplementary Code -  Script 4/5\n",
    "### Training and evaluating the convolutional partly quantized, one-hot-output CIFAR network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "cm = 1/2.54  # centimeters to inches\n",
    "\n",
    "import nninfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment id\n",
    "experiment_id = \"cifar10\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up parameters and train first network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we do not set initial seeds manually here, but save all seeds to the\n",
    "# checkpoints files during training for later reproducibility. Rerunning this script\n",
    "# will produce slightly different figures due to the randomness of network\n",
    "# initialization etc.\n",
    "\n",
    "# Set network architecture\n",
    "# Input size is 3 * 32 * 32\n",
    "# Then three convolutional layers with 32, 64 and 128 filters\n",
    "# Then a fully connected network part with 2048, 128, 32, 5, 5 and finally 10 neurons\n",
    "layer_infos = [\n",
    "    nninfo.LayerInfo(connection_layer='input', activation_function='input'),\n",
    "\n",
    "    nninfo.LayerInfo(connection_layer='conv2d', connection_layer_kwargs={'in_channels': 3, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': (1, 1)}, activation_function='relu'),\n",
    "    nninfo.LayerInfo(connection_layer='maxpool2d', connection_layer_kwargs={'kernel_size': 2}, activation_function=None),\n",
    "    \n",
    "    nninfo.LayerInfo(connection_layer='conv2d', connection_layer_kwargs={'in_channels': 32, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': (1, 1)}, activation_function='relu'),\n",
    "    nninfo.LayerInfo(connection_layer='maxpool2d', connection_layer_kwargs={'kernel_size': 2}, activation_function=None),\n",
    "\n",
    "    nninfo.LayerInfo(connection_layer='conv2d', connection_layer_kwargs={'in_channels': 32, 'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': (1, 1)}, activation_function='relu'),\n",
    "    nninfo.LayerInfo(connection_layer='maxpool2d', connection_layer_kwargs={'kernel_size': 2}, activation_function=None),\n",
    "\n",
    "    nninfo.LayerInfo(connection_layer='flatten', activation_function=None),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 1024, 'out_features': 2048}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 2048, 'out_features': 128}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 128, 'out_features': 32}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 32, 'out_features': 5}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 5, 'out_features': 5}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 5, 'out_features': 10}, activation_function='softmax_output'),\n",
    "]\n",
    "\n",
    "# Set weight initialization\n",
    "initializer_name = 'xavier'\n",
    "\n",
    "\n",
    "# Create network instance\n",
    "network = nninfo.NeuralNetwork(layer_infos=layer_infos,\n",
    "                               init_str=initializer_name)\n",
    "\n",
    "# Set task instance\n",
    "task = nninfo.TaskManager('cifar10_1d_dat')\n",
    "# Split into train and test set\n",
    "task['full_set'].train_test_val_sequential_split(50000, 10000, 0)\n",
    "\n",
    "# Create quantizer list with stochastic quantization. The input and convolutional layers are not quantized.\n",
    "quantizer = 8 * [None] + 5 * [{'levels': 8, 'rounding_point': 'stochastic'}] + [None]\n",
    "\n",
    "# Initialize training components\n",
    "trainer = nninfo.Trainer(dataset_name='full_set/train',\n",
    "                         optim_str='SGD',\n",
    "                         loss_str='CELoss',\n",
    "                         lr=0.005,\n",
    "                         shuffle=True,\n",
    "                         batch_size=64,\n",
    "                         quantizer=quantizer)\n",
    "\n",
    "tester = nninfo.Tester(dataset_name='full_set/test')\n",
    "\n",
    "schedule = nninfo.Schedule()\n",
    "\n",
    "# Save training state for 30 logarithmically spaced checkpoints\n",
    "schedule.create_log_spaced_chapters(1000, 30)\n",
    "\n",
    "# Combine components into experiment\n",
    "experiment = nninfo.Experiment(experiment_id=experiment_id,\n",
    "                        network=network,\n",
    "                        task=task,\n",
    "                        trainer=trainer,\n",
    "                        tester=tester,\n",
    "                        schedule=schedule)\n",
    "\n",
    "# Run training for 10^5 epochs\n",
    "experiment.run_following_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up experiment\n",
    "exp = nninfo.exp.Experiment(experiment_id, load=True)\n",
    "\n",
    "# Compute 9 more training runs with different random weight initializations.\n",
    "exp.rerun(9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate network performance\n",
    "#### Compute loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_params = 8 * [None] + 5 * [{'levels': 8, 'rounding_point': 'center_saturating'}] + [None]\n",
    "\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "performance_measurement = nninfo.analysis.PerformanceMeasurement(experiment, ['full_set/train', 'full_set/test'], quantizer_params=quantizer_params)\n",
    "\n",
    "performance_measurement.perform_measurements(run_ids='all', chapter_ids='all', exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance file\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "performance_measurement = nninfo.analysis.PerformanceMeasurement.load(experiment, \"performance\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4*cm, 4*cm), dpi=150)\n",
    "ax.set_ylim(0, 1)\n",
    "twinax = ax.twinx()\n",
    "\n",
    "# Plot accuracy\n",
    "nninfo.plot.plot_loss_accuracy(performance_measurement.results, ax, twinax)\n",
    "\n",
    "ax.legend(ncol=1, bbox_to_anchor=(1.5, 0.5), loc='center left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "plt.savefig(f\"experiments/exp_{experiment_id}/plots/performance.pdf\", bbox_inches='tight');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform quintivariate PID on hidden layers $L_3$, $L_4$ and $L_5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "exp = nninfo.Experiment.load(experiment_id)\n",
    "\n",
    "for layer in [10, 11]:\n",
    "\n",
    "    target = [nninfo.NeuronID('Y', (1,))]\n",
    "    source1 = [nninfo.NeuronID(f'L{layer}', (1,))]\n",
    "    source2 = [nninfo.NeuronID(f'L{layer}', (2,))]\n",
    "    source3 = [nninfo.NeuronID(f'L{layer}', (3,))]\n",
    "    source4 = [nninfo.NeuronID(f'L{layer}', (4,))]\n",
    "    source5 = [nninfo.NeuronID(f'L{layer}', (5,))]\n",
    "\n",
    "    # Create quantizer list for deterministic rounding.\n",
    "    quantizer = 8 * [None] + 5 * [{'levels': 8, 'dequant_point': 'center_saturating'}]\n",
    "\n",
    "    # Compute PID for all chapters of all random network initializations\n",
    "    pid_measurement = nninfo.analysis.PIDMeasurement(experiment,\n",
    "                                                     measurement_id=f'pid_L{layer}',\n",
    "                                                     dataset_name='full_set/train',\n",
    "                                                     pid_definition='sxpid',\n",
    "                                                     target_id_list=target,\n",
    "                                                     source_id_lists=[source1, source2, source3, source4, source5],\n",
    "                                                     binning_kwargs={'binning_method':'none'},\n",
    "                                                     quantizer_params=quantizer_params)\n",
    "\n",
    "    itic = time.time_ns()\n",
    "    pid_measurement.perform_measurements(run_ids='all', chapter_ids='all')\n",
    "    itoc = time.time_ns()\n",
    "    print(f\"Computing PID for L{layer} took: \", (itoc-itic)/10**9, \"s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute representational complexity and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nninfo.postprocessing.pid_postprocessing import get_pid_summary_quantities\n",
    "\n",
    "cm = 1/2.54\n",
    "fig, ax= plt.subplots(figsize=(5*cm, 4*cm), dpi=150)\n",
    "inset_axis = inset_axes(ax, width=0.7, height=0.4)\n",
    "\n",
    "# Load experiment\n",
    "experiment = nninfo.experiment.Experiment.load(experiment_id)\n",
    "\n",
    "# Plot accuracy on inset axis\n",
    "performance_measurement = nninfo.analysis.PerformanceMeasurement.load(experiment, 'performance')\n",
    "nninfo.plot.plot_accuracy(performance_measurement.results, 'full_set/train', inset_axis, c='k', label='Train')\n",
    "nninfo.plot.plot_accuracy(performance_measurement.results, 'full_set/test', inset_axis, c='k', ls='--', label='Test')\n",
    "inset_axis.set_ylim(0, 1)\n",
    "inset_axis.set_ylabel('Acc.')\n",
    "inset_axis.legend(bbox_to_anchor=(1.1, 1.3), loc='upper left')\n",
    "    \n",
    "for layer in [10, 11]:\n",
    "    measurement_loaded = nninfo.analysis.PIDMeasurement.load(experiment=experiment, measurement_id=f'pid_L{layer}')\n",
    "    pid_summary = get_pid_summary_quantities(measurement_loaded.results)\n",
    "    nninfo.plot.plot_representational_complexity(pid_summary, ax, label=f'Layer {layer}')\n",
    "\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.25))\n",
    "\n",
    "ax.set_xlabel('Training Epoch')\n",
    "ax.set_ylabel(r'Repr. Compl. $C$')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1, .6), loc='upper left')\n",
    "\n",
    "ax.set_ylim(1, 4)\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['1', '2', '3']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "fig.savefig(f\"experiments/exp_{experiment_id}/plots/representational_complexity.pdf\", bbox_inches='tight');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('nninfo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb676ebef6b5cf8ce51c716d153b38a6d674dadf41aeaecb86761f2bdfa1f1da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
