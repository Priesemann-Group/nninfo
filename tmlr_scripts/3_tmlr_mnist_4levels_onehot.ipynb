{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Measure of the Complexity of Neural Representations based on Partial Information Decomposition\n",
    "David A. Ehrlich, Andreas C. Schneider, Viola Priesemann, Michael Wibral, Abdullah Makkeh. TMLR 2023.\\\n",
    "Supplementary Code -  Script 3/5\n",
    "### Training and evaluating the 4-level-quantized, one-hot-output MNIST network (Figure 5.B,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import  math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "cm = 1/2.54  # centimeters to inches\n",
    "\n",
    "import nninfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment id\n",
    "experiment_id = \"mnist_4levels_onehot\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up parameters and train first network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we do not set initial seeds manually here, but save all seeds to the\n",
    "# checkpoints files during training for later reproducibility. Rerunning this script\n",
    "# will produce slightly different figures due to the randomness of network\n",
    "# initialization etc.\n",
    "\n",
    "layer_infos = [\n",
    "    nninfo.LayerInfo(connection_layer='input', activation_function='input'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 784, 'out_features': 50}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 50, 'out_features': 10}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 10, 'out_features': 5}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 5, 'out_features': 5}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 5, 'out_features': 5}, activation_function='tanh'),\n",
    "    nninfo.LayerInfo(connection_layer='linear', connection_layer_kwargs={'in_features': 5, 'out_features': 10}, activation_function='softmax_output')\n",
    "]\n",
    "\n",
    "# Set weight initialization\n",
    "initializer_name = 'xavier'\n",
    "\n",
    "# Create network instance\n",
    "network = nninfo.NeuralNetwork(layer_infos=layer_infos, init_str=initializer_name)\n",
    "\n",
    "# Set task instance\n",
    "task = nninfo.TaskManager('mnist_1d_dat')\n",
    "# Split dataset into MNIST training set, and MNIST+QMNIST test set\n",
    "task['full_set'].train_test_val_sequential_split(60_000, 60_000, 0)\n",
    "\n",
    "# Create quantizer list with stochastic quantization. The input layer is not quantized.\n",
    "quantizer = [None] + 5 * [{'levels': 4, 'rounding_point': 'stochastic'}] + [None]\n",
    "\n",
    "#Set up trainer\n",
    "trainer = nninfo.Trainer(dataset_name='full_set/train',\n",
    "                        optim_str='SGD',\n",
    "                        loss_str='CELoss',\n",
    "                        lr=0.01,\n",
    "                        shuffle=True,\n",
    "                        batch_size=64,\n",
    "                        quantizer=quantizer)\n",
    "\n",
    "# Set up tester\n",
    "tester = nninfo.Tester(dataset_name='full_set/test')\n",
    "\n",
    "# Set up schedule\n",
    "schedule = nninfo.Schedule()\n",
    "# Save training state for 50 logarithmically spaced checkpoints\n",
    "schedule.create_log_spaced_chapters(100_000, 50)\n",
    "\n",
    "# Combine components into experiment\n",
    "experiment = nninfo.Experiment(experiment_id=experiment_id,\n",
    "                        network=network,\n",
    "                        task=task,\n",
    "                        trainer=trainer,\n",
    "                        tester=tester,\n",
    "                        schedule=schedule)\n",
    "\n",
    "# Run training for 10^5 epochs\n",
    "experiment.run_following_schedule()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun training with different random weight initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up experiment\n",
    "exp = nninfo.Experiment.load(experiment_id)\n",
    "\n",
    "# Compute 19 more training runs with different random weight initializations.\n",
    "exp.rerun(19)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate network performance\n",
    "#### Compute loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_params = [None] + 5 * [{'levels': 4, 'rounding_point': 'center_saturating'}] + [None]\n",
    "\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "performance_measurement = nninfo.analysis.PerformanceMeasurement(experiment, ['full_set/train', 'full_set/test'], quantizer_params=quantizer_params)\n",
    "\n",
    "performance_measurement.perform_measurements(run_ids='all', chapter_ids='all', exists_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance file\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "performance_measurement = nninfo.analysis.PerformanceMeasurement.load(experiment, \"performance\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4*cm, 4*cm), dpi=150)\n",
    "ax.set_ylim(0, 1)\n",
    "twinax = ax.twinx()\n",
    "\n",
    "# Plot accuracy\n",
    "nninfo.plot.plot_loss_accuracy(performance_measurement.results, ax, twinax)\n",
    "\n",
    "ax.legend(ncol=1, bbox_to_anchor=(1.5, 0.5), loc='center left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "plt.savefig(f\"experiments/exp_{experiment_id}/plots/performance.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform subsampling PID on hidden layer $L_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_indices(n_sources: int, combination_index: int, layer_width: int):\n",
    "    \"\"\" Get deterministic random source indices for a given combination index.\"\"\"\n",
    "\n",
    "    # Create a reproducable shuffled list of numbers from 0 to (layer_width over n_sources)\n",
    "    np.random.seed(1234)\n",
    "    rand = np.random.permutation(math.comb(layer_width, n_sources))\n",
    "\n",
    "    # Get the combination_index'th element of the shuffled list\n",
    "    random_combination_index = rand[combination_index]\n",
    "\n",
    "    # Get the source indices from the random combination index\n",
    "    combinations_iter = itertools.combinations(range(layer_width), n_sources)\n",
    "    source_indices = next(x for i, x in enumerate(combinations_iter) if i == random_combination_index)\n",
    "\n",
    "    return source_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "\n",
    "# Repeat for 26 random draws of five neurons as sources\n",
    "for combination_index in range(26):\n",
    "\n",
    "    source_indices = get_source_indices(\n",
    "            n_sources=5, combination_index=combination_index, layer_width=10)\n",
    "\n",
    "    target = [nninfo.NeuronID('Y', (1,))]\n",
    "    sources = [[nninfo.NeuronID('L2', (source_id+1,))] for source_id in source_indices]\n",
    "\n",
    "    # Create quantizer list for deterministic rounding.\n",
    "    quantization_params = [None] + 5 * [{'levels': 4, 'rounding_point': 'center_saturating'}] + [None]\n",
    "\n",
    "    # Compute PID for all chapters of all random network initializations\n",
    "    pid_measurement = nninfo.analysis.PIDMeasurement(experiment,\n",
    "                                                     measurement_id=f'subsampling_pid_{source_indices}',\n",
    "                                                     dataset_name='full_set/train',\n",
    "                                                     pid_definition='sxpid',\n",
    "                                                     target_id_list=target,\n",
    "                                                     source_id_lists=sources,\n",
    "                                                     binning_kwargs={'binning_method':'none'},\n",
    "                                                     quantizer_params=quantizer_params)\n",
    "\n",
    "    itic = time.time_ns()\n",
    "    pid_measurement.perform_measurements(run_ids='all', chapter_ids='all')\n",
    "    itoc = time.time_ns()\n",
    "    print(f\"Computing subsampled PID for a single choice of sources in L2 took: \", (itoc-itic)/10**9, \"s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform coarse-grained PID on hidden layer $L_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "experiment = nninfo.Experiment.load(experiment_id)\n",
    "\n",
    "target = [nninfo.NeuronID('Y', (1,))]\n",
    "source1 = [nninfo.NeuronID(f'L2', (1,)), nninfo.NeuronID(f'L2', (2,))]\n",
    "source2 = [nninfo.NeuronID(f'L2', (3,)), nninfo.NeuronID(f'L2', (4,))]\n",
    "source3 = [nninfo.NeuronID(f'L2', (5,)), nninfo.NeuronID(f'L2', (6,))]\n",
    "source4 = [nninfo.NeuronID(f'L2', (7,)), nninfo.NeuronID(f'L2', (8,))]\n",
    "source5 = [nninfo.NeuronID(f'L2', (9,)), nninfo.NeuronID(f'L2', (10,))]\n",
    "# Create quantizer list for deterministic rounding.\n",
    "quantization_dict = [None] + 5 * [{'levels': 4, 'roundidng_point': 'center_saturating'}] + [None]\n",
    "\n",
    "pid_measurement = nninfo.analysis.PIDMeasurement(experiment,\n",
    "                                                     measurement_id=f'coarse_graining_pid',\n",
    "                                                     dataset_name='full_set/train',\n",
    "                                                     pid_definition='sxpid',\n",
    "                                                     target_id_list=target,\n",
    "                                                     source_id_lists=sources,\n",
    "                                                     binning_kwargs={'binning_method':None},\n",
    "                                                     quantizer_params=quantizer_params)\n",
    "\n",
    "itic = time.time_ns()\n",
    "pid_measurement.perform_measurements(run_ids='all', chapter_ids='all')\n",
    "itoc = time.time_ns()\n",
    "print(f\"Computing coarse-grained PID for L2 took: \", (itoc-itic)/10**9, \"s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute representational complexity for subsampling and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nninfo.postprocessing.pid_postprocessing import get_pid_summary_quantities\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4*cm, 4*cm), sharex=True, dpi=150)\n",
    "\n",
    "# Combine result dataframes from the different subsampling measurements\n",
    "results = pd.DataFrame()\n",
    "for combination_index in range(26):\n",
    "    measurement = nninfo.analysis.PIDMeasurement.load(experiment, f'subsampling_pid_{combination_index}')\n",
    "    pid_summary = get_pid_summary_quantities(measurement.results)\n",
    "    results = pd.concat([results, pid_summary])\n",
    "\n",
    "# Plot the results\n",
    "nninfo.plot.plot_representational_complexity(results, ax, label='$L_2$')\n",
    "\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.25))\n",
    "\n",
    "ax.set_xlabel('Training Epoch')\n",
    "ax.set_ylabel(r'Repr. Compl. $C$')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_ylim(1.25, 3.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "plt.savefig(f\"experiments/exp_{experiment_id}/plots/representational_complexity_subsampling.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute representational complexity for coarse-graining and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nninfo.postprocessing.pid_postprocessing import get_pid_summary_quantities\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4*cm, 4*cm), sharex=True, dpi=150)\n",
    "\n",
    "# Combine result dataframes from the different subsampling measurements\n",
    "measurement = nninfo.analysis.PIDMeasurement.load(experiment, f'coarse_graining_pid')\n",
    "pid_summary = get_pid_summary_quantities(measurement.results)\n",
    "\n",
    "# Plot the results\n",
    "nninfo.plot.plot_representational_complexity(pid_summary, ax, label='$L_2$')\n",
    "\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.25))\n",
    "\n",
    "ax.set_xlabel('Training Epoch')\n",
    "ax.set_ylabel(r'Repr. Compl. $C$')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_ylim(1.25, 3.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "plt.savefig(f\"experiments/exp_{experiment_id}/plots/representational_complexity_coarse_graining.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nninfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb676ebef6b5cf8ce51c716d153b38a6d674dadf41aeaecb86761f2bdfa1f1da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
